{
  
    
        "post0": {
            "title": "In this mini-project, we will assess major-minor axis ratio and alignment of nuclei in H&E histopathology images of Soft Tissue Leiomyosarcoma",
            "content": "import histomicstk as htk import numpy as np import scipy as sp import skimage.io import skimage.measure import skimage.color import matplotlib.pyplot as plt import matplotlib.patches as mpatches %matplotlib inline # Some nice default configuration for plots plt.rcParams[&#39;figure.figsize&#39;] = 10, 10 plt.rcParams[&#39;image.cmap&#39;] = &#39;gray&#39; titlesize = 24 . longitudinal_image_file = &#39;longitudinal.png&#39; long_im_input = skimage.io.imread(longitudinal_image_file)[:,:,:3] plt.imshow(long_im_input) _ = plt.title(&#39;Longitudinal Cellular Orientation&#39;, fontsize=16) . transverse_image_file = &#39;transverse.png&#39; trans_im_input = skimage.io.imread(transverse_image_file)[:,:,:3] plt.imshow(trans_im_input) _ = plt.title(&#39;Transverse Cellular Orientation&#39;, fontsize=16) . ref_image_file = (&#39;6070-7712.png&#39;) # L1.png im_reference = skimage.io.imread(ref_image_file)[:,:,:3] # get mean and stddev of reference image in lab space mean_ref, std_ref = htk.preprocessing.color_conversion.lab_mean_std(im_reference) . long_im_nmzd = htk.preprocessing.color_normalization.reinhard(long_im_input, mean_ref, std_ref) trans_im_nmzd = htk.preprocessing.color_normalization.reinhard(trans_im_input, mean_ref, std_ref) . plt.figure(figsize=(20, 10)) plt.subplot(1, 2, 1) plt.imshow(im_reference) _ = plt.title(&#39;Reference Image&#39;, fontsize=titlesize) plt.subplot(1, 2, 2) plt.imshow(long_im_nmzd) _ = plt.title(&#39;Normalized Longitudinal Input Image&#39;, fontsize=titlesize) . plt.figure(figsize=(20, 10)) plt.subplot(1, 2, 1) plt.imshow(im_reference) _ = plt.title(&#39;Reference Image&#39;, fontsize=titlesize) plt.subplot(1, 2, 2) plt.imshow(trans_im_nmzd) _ = plt.title(&#39;Normalized Transverse Input Image&#39;, fontsize=titlesize) . # create stain to color map stainColorMap = { &#39;hematoxylin&#39;: [0.65, 0.70, 0.29], &#39;eosin&#39;: [0.07, 0.99, 0.11], &#39;dab&#39;: [0.27, 0.57, 0.78], &#39;null&#39;: [0.0, 0.0, 0.0] } # specify stains of input image stain_1 = &#39;hematoxylin&#39; # nuclei stain stain_2 = &#39;eosin&#39; # cytoplasm stain stain_3 = &#39;null&#39; # set to null if input contains only two stains # create stain matrix W = np.array([stainColorMap[stain_1], stainColorMap[stain_2], stainColorMap[stain_3]]).T . long_im_stains = htk.preprocessing.color_deconvolution.color_deconvolution(long_im_nmzd, W).Stains # Display results plt.figure(figsize=(20, 10)) plt.subplot(1, 2, 1) plt.imshow(long_im_stains[:, :, 0]) plt.title(stain_1, fontsize=titlesize) plt.subplot(1, 2, 2) plt.imshow(long_im_stains[:, :, 1]) _ = plt.title(stain_2, fontsize=titlesize) . trans_im_stains = htk.preprocessing.color_deconvolution.color_deconvolution(trans_im_nmzd, W).Stains # Display results plt.figure(figsize=(20, 10)) plt.subplot(1, 2, 1) plt.imshow(trans_im_stains[:, :, 0]) plt.title(stain_1, fontsize=titlesize) plt.subplot(1, 2, 2) plt.imshow(trans_im_stains[:, :, 1]) _ = plt.title(stain_2, fontsize=titlesize) . # get nuclei/hematoxylin channel long_im_nuclei_stain = long_im_stains[:, :, 0] trans_im_nuclei_stain = trans_im_stains[:, :, 0] # segment foreground foreground_threshold = 60 long_im_fgnd_mask = sp.ndimage.morphology.binary_fill_holes( long_im_nuclei_stain &lt; foreground_threshold) trans_im_fgnd_mask = sp.ndimage.morphology.binary_fill_holes( trans_im_nuclei_stain &lt; foreground_threshold) # run adaptive multi-scale LoG filter min_radius = 10 max_radius = 15 long_im_log_max, long_im_sigma_max = htk.filters.shape.cdog( long_im_nuclei_stain, long_im_fgnd_mask, sigma_min=min_radius * np.sqrt(2), sigma_max=max_radius * np.sqrt(2) ) trans_im_log_max, trans_im_sigma_max = htk.filters.shape.cdog( trans_im_nuclei_stain, trans_im_fgnd_mask, sigma_min=min_radius * np.sqrt(2), sigma_max=max_radius * np.sqrt(2) ) # detect and segment nuclei using local maximum clustering local_max_search_radius = 10 long_im_nuclei_seg_mask, seeds, maxima = htk.segmentation.nuclear.max_clustering( long_im_log_max, long_im_fgnd_mask, local_max_search_radius) trans_im_nuclei_seg_mask, seeds, maxima = htk.segmentation.nuclear.max_clustering( trans_im_log_max, trans_im_fgnd_mask, local_max_search_radius) # filter out small objects min_nucleus_area = 80 long_im_nuclei_seg_mask = htk.segmentation.label.area_open( long_im_nuclei_seg_mask, min_nucleus_area).astype(np.int) trans_im_nuclei_seg_mask = htk.segmentation.label.area_open( trans_im_nuclei_seg_mask, min_nucleus_area).astype(np.int) # compute nuclei properties LongObjProps = skimage.measure.regionprops(long_im_nuclei_seg_mask) TransObjProps = skimage.measure.regionprops(trans_im_nuclei_seg_mask) print(&#39;Number of nuclei in longitudinal image = &#39;, len(LongObjProps)) print(&#39;Number of nuclei in transverse image = &#39;, len(TransObjProps)) . Number of nuclei in longitudinal image = 43 Number of nuclei in transverse image = 122 . plt.figure(figsize=(20, 10)) plt.subplot(1, 2, 1) plt.imshow(skimage.color.label2rgb(long_im_nuclei_seg_mask, long_im_input, bg_label=0), origin=&#39;lower&#39;) plt.title(&#39;Nuclei segmentation mask overlay&#39;, fontsize=titlesize) plt.subplot(1, 2, 2) plt.imshow( long_im_input ) plt.xlim([0, long_im_input.shape[1]]) plt.ylim([0, long_im_input.shape[0]]) plt.title(&#39;Nuclei bounding boxes&#39;, fontsize=titlesize) for i in range(len(LongObjProps)): c = [LongObjProps[i].centroid[1], LongObjProps[i].centroid[0], 0] width = LongObjProps[i].bbox[3] - LongObjProps[i].bbox[1] + 1 height = LongObjProps[i].bbox[2] - LongObjProps[i].bbox[0] + 1 cur_bbox = { &quot;type&quot;: &quot;rectangle&quot;, &quot;center&quot;: c, &quot;width&quot;: width, &quot;height&quot;: height, } plt.plot(c[0], c[1], &#39;g+&#39;) mrect = mpatches.Rectangle([c[0] - 0.5 * width, c[1] - 0.5 * height] , width, height, fill=False, ec=&#39;g&#39;, linewidth=2) plt.gca().add_patch(mrect) . plt.figure(figsize=(20, 10)) plt.subplot(1, 2, 1) plt.imshow(skimage.color.label2rgb(trans_im_nuclei_seg_mask, trans_im_input, bg_label=0), origin=&#39;lower&#39;) plt.title(&#39;Nuclei segmentation mask overlay&#39;, fontsize=titlesize) plt.subplot(1, 2, 2) plt.imshow( trans_im_input ) plt.xlim([0, trans_im_input.shape[1]]) plt.ylim([0, trans_im_input.shape[0]]) plt.title(&#39;Nuclei bounding boxes&#39;, fontsize=titlesize) for i in range(len(TransObjProps)): c = [TransObjProps[i].centroid[1], TransObjProps[i].centroid[0], 0] width = TransObjProps[i].bbox[3] - TransObjProps[i].bbox[1] + 1 height = TransObjProps[i].bbox[2] - TransObjProps[i].bbox[0] + 1 cur_bbox = { &quot;type&quot;: &quot;rectangle&quot;, &quot;center&quot;: c, &quot;width&quot;: width, &quot;height&quot;: height, } plt.plot(c[0], c[1], &#39;g+&#39;) mrect = mpatches.Rectangle([c[0] - 0.5 * width, c[1] - 0.5 * height] , width, height, fill=False, ec=&#39;g&#39;, linewidth=2) plt.gca().add_patch(mrect) . LongObjPropsTable = skimage.measure.regionprops_table(long_im_nuclei_seg_mask, properties=(&#39;label&#39;, &#39;orientation&#39;, &#39;major_axis_length&#39;, &#39;minor_axis_length&#39;,) ) . TransObjPropsTable = skimage.measure.regionprops_table(trans_im_nuclei_seg_mask, properties=(&#39;label&#39;, &#39;orientation&#39;, &#39;major_axis_length&#39;, &#39;minor_axis_length&#39;) ) . long_orientation = LongObjPropsTable[&#39;orientation&#39;] trans_orientation = TransObjPropsTable[&#39;orientation&#39;] . _ = plt.hist(long_orientation) plt.show() . _ = plt.hist(trans_orientation) plt.show() . LongObjPropsTable[&#39;minor_axis_length&#39;] . array([17.82988548, 26.57283721, 32.62511918, 17.9561122 , 24.04859764, 25.90507908, 40.4939446 , 35.97834554, 22.36994041, 14.45953977, 40.12733974, 21.8516542 , 12.42029426, 23.54320991, 20.64609371, 24.50122929, 31.37118343, 23.3645544 , 18.90967646, 28.31049082, 27.86635531, 15.762809 , 32.84102401, 31.56465618, 18.39091073, 39.147678 , 24.17782857, 20.93425473, 24.76587761, 34.92536675, 33.35863685, 23.92101848, 25.98666127, 30.92181968, 17.00492511, 27.91541626, 44.926728 , 16.78490142, 54.96896025, 28.69640483, 20.71021864, 16.72920149, 18.5068537 ]) .",
            "url": "https://https//jamcron.github.io//research_blog/2021/12/17/Nuclei_Orientation.html",
            "relUrl": "/2021/12/17/Nuclei_Orientation.html",
            "date": " • Dec 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Plot Tumor Differentiation Score Analysis",
            "content": "%matplotlib inline import matplotlib.pyplot as plt; plt.rcdefaults() import numpy as np import matplotlib.pyplot as plt SMALL_SIZE = 8 MEDIUM_SIZE = 10 BIGGER_SIZE = 12 scores = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] missed = [0.123287671232877, 0.125933267261315, 0.003718728870859] plt.bar(scores, missed) plt.ylabel(&quot;Error Rate&quot;) plt.xlabel(&quot;Tumor Differentiation Score&quot;) plt.title(&quot;Error rates by Tumor Differentiation Score&quot;) # plt.rc(&#39;font&#39;, size=SMALL_SIZE) # controls default text sizes # plt.rc(&#39;axes&#39;, titlesize=SMALL_SIZE) # fontsize of the axes title # plt.rc(&#39;axes&#39;, labelsize=MEDIUM_SIZE) # fontsize of the x and y labels # plt.rc(&#39;xtick&#39;, labelsize=SMALL_SIZE) # fontsize of the tick labels # plt.rc(&#39;ytick&#39;, labelsize=SMALL_SIZE) # fontsize of the tick labels # plt.rc(&#39;legend&#39;, fontsize=SMALL_SIZE) # legend fontsize # plt.rc(&#39;figure&#39;, titlesize=BIGGER_SIZE) # fontsize of the figure title plt.show() . %matplotlib inline import matplotlib.pyplot as plt; plt.rcdefaults() import numpy as np import matplotlib.pyplot as plt SMALL_SIZE = 8 MEDIUM_SIZE = 10 BIGGER_SIZE = 12 scores = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] missed = [368/3869, 1114/15135, 0/2958] plt.bar(scores, missed) plt.ylabel(&quot;Error Rate&quot;) plt.xlabel(&quot;Tumor Differentiation Score&quot;) plt.title(&quot;Error Rates by Tumor Differentiation Score&quot;, fontsize=14, fontweight=1000 ) # plt.rc(&#39;font&#39;, size=SMALL_SIZE) # controls default text sizes # plt.rc(&#39;axes&#39;, titlesize=SMALL_SIZE) # fontsize of the axes title # plt.rc(&#39;axes&#39;, labelsize=MEDIUM_SIZE) # fontsize of the x and y labels # plt.rc(&#39;xtick&#39;, labelsize=SMALL_SIZE) # fontsize of the tick labels # plt.rc(&#39;ytick&#39;, labelsize=SMALL_SIZE) # fontsize of the tick labels # plt.rc(&#39;legend&#39;, fontsize=SMALL_SIZE) # legend fontsize # plt.rc(&#39;figure&#39;, titlesize=BIGGER_SIZE) # fontsize of the figure title plt.show() . Plot normal tissue structure analysis . import pandas as pd from matplotlib import pyplot as plt data = [[&#39;Smooth Muscle&#39;, 0.147, 0.681], [&#39;Fibroblasts &amp; Collagen&#39;, 0.825, 0.639], [&#39;Adipose&#39;, 0.233, 0.032], # [&#39;Inflammatory Aggregates&#39;, 0.041, 0.084], # [&#39;Arteries&#39;, 0.127, 0.065], #[&#39;Veins &amp; Lymphatics&#39;, 0.080, 0.155] ] df = pd.DataFrame(data, columns = [&#39;Tissue Structure&#39;, &#39;Correctly Classified as Normal (n=464)&#39;, &#39;Incorrectly Classified as Tumor (n=476)&#39; ] ) print(df) df.plot(x = &quot;Tissue Structure&quot;, y = [&#39;Correctly Classified as Normal (n=464)&#39;,&#39;Incorrectly Classified as Tumor (n=476)&#39;], kind= &#39;bar&#39;, figsize=(10,7) ) plt.title(&quot;Normal Tissue Structure Analysis&quot;, fontsize=20, fontweight=1000) plt.ylabel(&quot;Percentage of Tiles Containing the Histologic Feature&quot;, fontsize=14, rotation=90) plt.xlabel(None) plt.xticks(rotation=0, fontsize=14) plt.legend(prop={&#39;size&#39;:11.5}) plt.show() . Tissue Structure Correctly Classified as Normal (n=464) 0 Smooth Muscle 0.147 1 Fibroblasts &amp; Collagen 0.825 2 Adipose 0.233 Incorrectly Classified as Tumor (n=476) 0 0.681 1 0.639 2 0.032 . Plot confusion matrix for tumor v. normal classifier . from mlxtend.plotting import plot_confusion_matrix . Actual_tumor_predicted_tumor = 16557 Actual_tumor_predicted_normal = 765 Actual_normal_predicted_normal = 3855 Actual_normal_predicted_tumor = 703 . fig, ax = plot_confusion_matrix(conf_mat=conf_matrix, figsize=(6, 6), cmap=plt.cm.Greens) plt.xlabel(&#39;Predictions&#39;, fontsize=18) plt.ylabel(&#39;Actuals&#39;, fontsize=18) plt.title(&#39;Confusion Matrix&#39;, fontsize=18) plt.show() . from mlxtend.evaluate import confusion_matrix cm =cm = confusion_matrix(y_target=targ_list, y_predicted=pred_list) cm import matplotlib.pyplot as plt from mlxtend.plotting import plot_confusion_matrix fig, ax = plot_confusion_matrix(conf_mat=cm) plt.show() . NameError Traceback (most recent call last) &lt;ipython-input-1-2c1f8f1a1481&gt; in &lt;module&gt; 1 from mlxtend.evaluate import confusion_matrix 2 -&gt; 3 cm =cm = confusion_matrix(y_target=targ_list, 4 y_predicted=pred_list) 5 cm NameError: name &#39;targ_list&#39; is not defined . diagram . # from matplotlib import pyplot as plt # data = [[&#39;Smooth Muscle&#39;, &#39;Correct classification&#39;, 0.147], # [&#39;Smooth Muscle&#39;, &#39;Misclassification&#39;, 0.681], # [&#39;Adipose&#39;, &#39;Correct classification&#39;, 0.233], # [&#39;Adipose&#39;, &#39;Misclassification&#39;, 0.032], # [&#39;Fibroblasts &amp; Collagen&#39;, &#39;Correct classification&#39;, 0.825], # [&#39;Fibroblasts &amp; Collagen&#39;, &#39;Misclassification&#39;, 0.639], # [&#39;Inflammatory Aggregates&#39;, &#39;Correct classification&#39;, 0.041], # [&#39;Inflammatory Aggregates&#39;, &#39;Misclassification&#39;, 0.084], # [&#39;Arteries&#39;, &#39;Correct classification&#39;, 0.127], # [&#39;Arteries&#39;, &#39;Misclassification&#39;, 0.065], # [&#39;Veins &amp; Lymphatics&#39;, &#39;Correct classification&#39;, 0.080], # [&#39;Veins &amp; Lymphatics&#39;, &#39;Misclassification&#39;, 0.155] # ] from mlxtend.evaluate import confusion_matrix cm =cm = confusion_matrix(y_target=targ_list, y_predicted=pred_list) cm import matplotlib.pyplot as plt from mlxtend.plotting import plot_confusion_matrix fig, ax = plot_confusion_matrix(conf_mat=cm) plt.show() # df = pd.DataFrame(data, columns = [&#39;Tissue Structure&#39;, &#39;Prediction&#39;, &#39;Frequency&#39;]) # df # df.plot(x = &quot;Tissue Structure&quot;, # y = [&#39;Prediction&#39;,&#39;Frequency&#39;], # kind= &#39;bar&#39;, # figsize=(9,8) # # ) # plt.show() # # colors = {&#39;Correct classification&#39;:&#39;green&#39;, &#39;Misclassification&#39;:&#39;blue&#39;} # c = data[&#39;Prediction&#39;].apply(lambda x: colors[x]) # df.plot(x=&quot;Tissue Structure&quot;, y=[&#39;Prediction&#39;,&quot;Frequency&quot;], kind=&quot;bar&quot;) # bars = plt.bar(x=data[&#39;Tissue Structure&#39;], y=[data[&#39;Frequency&#39;], str(data[&#39;Prediction&#39;])], color=c, label=colors, # height=1) # labels = list(colors.keys()) # handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels] # plt.legend(handles, labels) .",
            "url": "https://https//jamcron.github.io//research_blog/2021/12/16/Plots_and_Figs.html",
            "relUrl": "/2021/12/16/Plots_and_Figs.html",
            "date": " • Dec 16, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I go from skeptic to a zealot of literate programming? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "https://https//jamcron.github.io//research_blog/codespaces",
            "relUrl": "/codespaces",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "GitHub Actions: Providing Data Scientists With New Superpowers",
            "content": "What Superpowers? . Hi, I’m Hamel Husain. I’m a machine learning engineer at GitHub. Recently, GitHub released a new product called GitHub Actions, which has mostly flown under the radar in the machine learning and data science community as just another continuous integration tool. . Recently, I’ve been able to use GitHub Actions to build some very unique tools for Data Scientists, which I want to share with you today. Most importantly, I hope to get you excited about GitHub Actions, and the promise it has for giving you new superpowers as a Data Scientist. Here are two projects I recently built with Actions that show off its potential: . fastpages . fastpages is an automated, open-source blogging platform with enhanced support for Jupyter notebooks. You save your notebooks, markdown, or Word docs into a directory on GitHub, and they automatically become blog posts. Read the announcement below: . We&#39;re launching `fastpages`, a platform which allows you to host a blog for free, with no ads. You can blog with @ProjectJupyter notebooks, @office Word, directly from @github&#39;s markdown editor, etc.Nothing to install, &amp; setup is automated!https://t.co/dNSA0oQUrN . &mdash; Jeremy Howard (@jeremyphoward) February 24, 2020 Machine Learning Ops . Wouldn’t it be cool if you could invoke a chatbot natively on GitHub to test your machine learning models on the infrastructure of your choice (GPUs), log all the results, and give you a rich report back in a pull request so that everyone could see the results? You can with GitHub Actions! . Consider the below annotated screenshot of this Pull Request: . . A more in-depth explanation about the above project can be viewed in this video: . Using GitHub Actions for machine learning workflows is starting to catch on. Julien Chaumond, CTO of Hugging Face, says: . GitHub Actions are great because they let us do CI on GPUs (as most of our users use the library on GPUs not on CPUs), on our own infra! 1 . Additionally, you can host a GitHub Action for other people so others can use parts of your workflow without having to re-create your steps. I provide examples of this below. . A Gentle Introduction To GitHub Actions . What Are GitHub Actions? . GitHub Actions allow you to run arbitrary code in response to events. Events are activities that happen on GitHub such as: . Opening a pull request | Making an issue comment | Labeling an issue | Creating a new branch | … and many more | . When an event is created, the GitHub Actions context is hydrated with a payload containing metadata for that event. Below is an example of a payload that is received when an issue is created: . { &quot;action&quot;: &quot;created&quot;, &quot;issue&quot;: { &quot;id&quot;: 444500041, &quot;number&quot;: 1, &quot;title&quot;: &quot;Spelling error in the README file&quot;, &quot;user&quot;: { &quot;login&quot;: &quot;Codertocat&quot;, &quot;type&quot;: &quot;User&quot;, }, &quot;labels&quot;: [ { &quot;id&quot;: 1362934389, &quot;node_id&quot;: &quot;MDU6TGFiZWwxMzYyOTM0Mzg5&quot;, &quot;name&quot;: &quot;bug&quot;, } ], &quot;body&quot;: &quot;It looks like you accidently spelled &#39;commit&#39; with two &#39;t&#39;s.&quot; } . This functionality allows you to respond to various events on GitHub in an automated way. In addition to this payload, GitHub Actions also provide a plethora of variables and environment variables that afford easy to access metadata such as the username and the owner of the repo. Additionally, other people can package useful functionality into an Action that other people can inherit. For example, consider the below Action that helps you publish python packages to PyPi: . The Usage section describes how this Action can be used: . - name: Publish a Python distribution to PyPI uses: pypa/gh-action-pypi-publish@master with: user: __token__ password: ${{ secrets.pypi_password }} . This Action expects two inputs: user and a password. You will notice that the password is referencing a variable called secrets, which is a variable that contains an encrypted secret that you can upload to your GitHub repository. There are thousands of Actions (that are free) for a wide variety of tasks that can be discovered on the GitHub Marketplace. The ability to inherit ready-made Actions in your workflow allows you to accomplish complex tasks without implementing all of the logic yourself. Some useful Actions for those getting started are: . actions/checkout: Allows you to quickly clone the contents of your repository into your environment, which you often want to do. This does a number of other things such as automatically mount your repository’s files into downstream Docker containers. | mxschmitt/action-tmate: Proivdes a way to debug Actions interactively. This uses port forwarding to give you a terminal in the browser that is connected to your Actions runner. Be careful not to expose sensitive information if you use this. | actions/github-script: Gives you a pre-authenticated ocotokit.js client that allows you to interact with the GitHub API to accomplish almost any task on GitHub automatically. Only these endpoints are supported (for example, the secrets endpoint is not in that list). | . In addition to the aforementioned Actions, it is helpful to go peruse the official GitHub Actions docs before diving in. . Example: A fastpages Action Workflow . The best to way familiarize yourself with Actions is by studying examples. Let’s take a look at the Action workflow that automates the build of fastpages (the platform used to write this blog post). . Part 1: Define Workflow Triggers . First, we define triggers in ci.yaml. Like all Actions workflows, this is a YAML file located in the .github/workflows directory of the GitHub repo. . The top of this YAML file looks like this: . name: CI on: push: branches: - master pull_request: . This means that this workflow is triggered on either a push or pull request event. Furthermore, push events are filtered such that only pushes to the master branch will trigger the workflow, whereas all pull requests will trigger this workflow. It is important to note that pull requests opened from forks will have read-only access to the base repository and cannot access any secrets for security reasons. The reason for defining the workflow in this way is we wanted to trigger the same workflow to test pull requests as well as build and deploy the website when a PR is merged into master. This will be clarified as we step through the rest of the YAML file. . Part 2: Define Jobs . Next, we define jobs (there is only one in this workflow). Per the docs: . A workflow run is made up of one or more jobs. Jobs run in parallel by default. . jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: . The keyword build-site is the name of your job and you can name it whatever you want. In this case, we have a conditional if statement that dictates if this job should be run or not. We are trying to ensure that this workflow does not run when the first commit to a repo is made with the message ‘Initial commit’. The first variable in the if statement, github.event, contains a json payload of the event that triggered this workflow. When developing workflows, it is helpful to print this variable in order to inspect its structure, which you can accomplish with the following YAML: . - name: see payload run: | echo &quot;PAYLOAD: n${PAYLOAD} n&quot; env: PAYLOAD: ${{ toJSON(github.event) }} . Note: the above step is only for debugging and is not currently in the workflow. . toJson is a handy function that returns a pretty-printed JSON representation of the variable. The output is printed directly in the logs contained in the Actions tab of your repo. In this example, printing the payload for a push event will look like this (truncated for brevity): . { &quot;ref&quot;: &quot;refs/tags/simple-tag&quot;, &quot;before&quot;: &quot;6113728f27ae8c7b1a77c8d03f9ed6e0adf246&quot;, &quot;created&quot;: false, &quot;deleted&quot;: true, &quot;forced&quot;: false, &quot;base_ref&quot;: null, &quot;commits&quot;: [ { &quot;message&quot;: &quot;updated README.md&quot;, &quot;author&quot;: &quot;hamelsmu&quot; }, ], &quot;head_commit&quot;: null, } . Therefore, the variable github.event.commits[0].message will retrieve the first commit message in the array of commits. Since we are looking for situations where there is only one commit, this logic suffices. The second variable in the if statement, github.run_number is a special variable in Actions which: . [is a] unique number for each run of a particular workflow in a repository. This number begins at 1 for the workflow’s first run, and increments with each new run. This number does not change if you re-run the workflow run. . Therefore, the if statement introduced above: . if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 . Allows the workflow to run when the commit message is “Initial commit” as long as it is not the first commit. ( || is a logical or operator). . Finally, the line runs-on: ubuntu-latest specifies the host operating system that your workflows will run in. . Part 3: Define Steps . Per the docs: . A job contains a sequence of tasks called steps. Steps can run commands, run setup tasks, or run an Action in your repository, a public repository, or an Action published in a Docker registry. Not all steps run Actions, but all Actions run as a step. Each step runs in its own process in the runner environment and has access to the workspace and filesystem. Because steps run in their own process, changes to environment variables are not preserved between steps. GitHub provides built-in steps to set up and complete a job. . Below are the first two steps in our workflow: . - name: Copy Repository Contents uses: actions/checkout@master with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files . The first step creates a copy of your repository in the Actions file system, with the help of the utility action/checkout. This utility only fetches the last commit by default and saves files into a directory (whose path is stored in the environment variable GITHUB_WORKSPACE that is accessible by subsequent steps in your job. The second step runs the fastai/fastpages Action, which converts notebooks and word documents to blog posts automatically. In this case, the syntax: . uses: ./_action_files . is a special case where the pre-made GitHub Action we want to run happens to be defined in the same repo that runs this workflow. This syntax allows us to test changes to this pre-made Action when evaluating PRs by referencing the directory in the current repository that defines that pre-made Action. Note: Building pre-made Actions is beyond the scope of this tutorial. . The next three steps in our workflow are defined below: . - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;gem install bundler &amp;&amp; jekyll build -V&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : . The step named setup directories for Jekyll build executes shell commands that remove the _site folder in order to get rid of stale files related to the page we want to build, as well as grant permissions to all the files in our repo to subsequent steps. . The step named Jekyll build executes a docker container hosted by the Jekyll community on Dockerhub called jekyll/jekyll. For those not familiar with Docker, see this tutorial. The name of this container is called fastai/fastpages-jekyll because I’m adding some additional dependencies to jekyll/jekyll and hosting those on my DockerHub account for faster build times2. The args parameter allows you to execute arbitrary commands with the Docker container by overriding the CMD instruction in the Dockerfile. We use this Docker container hosted on Dockerhub so we don’t have to deal with installing and configuring all of the complicated dependencies for Jekyll. The files from our repo are already available in the Actions runtime due to the first step in this workflow, and are mounted into this Docker container automatically for us. In this case, we are running the command jekyll build, which builds our website and places relevant assets them into the _site folder. For more information about Jekyll, read the official docs. Finally, the env parameter allows me to pass an environment variable into the Docker container. . The final command above copies a CNAME file into the _site folder, which we need for the custom domain https://fastpages.fast.ai. Setting up custom domains are outside the scope of this article. . The final step in our workflow is defined below: . - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.SSH_DEPLOY_KEY }} publish_dir: ./_site . The statement . if: github.event_name == &#39;push&#39; . uses the variable github.event_name to ensure this step only runs when a push event ( in this case only pushes to the master branch trigger this workflow) occur. . This step deploys the fastpages website by copying the contents of the _site folder to the root of the gh-pages branch, which GitHub Pages uses for hosting. This step uses the peaceiris/actions-gh-pages Action, pinned at version 3. Their README describes various options and inputs for this Action. . Conclusion . We hope that this has shed some light on how we use GitHub Actions to automate fastpages. While we only covered one workflow above, we hope this provides enough intuition to understand the other workflows in fastpages. We have only scratched the surface of GitHub Actions in this blog post, but we provide other materials below for those who want to dive in deeper. We have not covered how to host an Action for other people, but you can start with these docs to learn more. . Still confused about how GitHub Actions could be used for Data Science? Here are some ideas of things you can build: . Jupyter Widgets that trigger GitHub Actions to perform various tasks on GitHub via the repository dispatch event | Integration with Pachyderm for data versioning. | Integration with your favorite cloud machine learning services, such Sagemaker, Azure ML or GCP’s AI Platform. | . Related Materials . GitHub Actions official documentation | Hello world Docker Action: A template to demonstrate how to build a Docker Action for other people to use. | Awesome Actions: A curated list of interesting GitHub Actions by topic. | A tutorial on Docker for Data Scientists. | . Getting In Touch . Please feel free to get in touch with us on Twitter: . Hamel Husain @HamelHusain | Jeremy Howard @jeremyphoward | . . Footnotes . You can see some of Hugging Face’s Actions workflows for machine learning on GitHub &#8617; . | These additional dependencies are defined here, which uses the “jekyll build” command to add ruby dedpendencies from the Gemfile located at the root of the repo. Additionally, this docker image is built by another Action workflow defined here. &#8617; . |",
            "url": "https://https//jamcron.github.io//research_blog/actions/markdown/2020/03/06/fastpages-actions.html",
            "relUrl": "/actions/markdown/2020/03/06/fastpages-actions.html",
            "date": " • Mar 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://https//jamcron.github.io//research_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://https//jamcron.github.io//research_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}